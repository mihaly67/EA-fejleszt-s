 #!/usr/bin/env python
# -*- coding: utf-8 -*-
import sys
import json
import os
import re
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
from rank_bm25 import BM25Okapi

# --- KONFIGUR√ÅCI√ì ---
MODEL_NAME = 'all-mpnet-base-v2'
MAX_STEPS = 3       # H√°ny l√©p√©s m√©lys√©gbe √°sson le?
TOP_K = 4           # H√°ny tal√°latot hozzon l√©p√©senk√©nt?

def log(msg):
    """L√°that√≥ visszajelz√©s."""
    print(f"   [√úGYN√ñK]: {msg}")

def load_resources():
    log("Mem√≥ria √©s Indexek bet√∂lt√©se...")
    docs = []
    search_roots = ['rag_theory', 'rag_code']

    # Ha nincsenek let√∂lt√∂tt mapp√°k, n√©zz√ºk a gy√∂keret is (h√°tha ott van a JSONL)
    if not os.path.exists('rag_theory') and not os.path.exists('rag_code'):
        search_roots.append('.')

    for root_dir in search_roots:
        if not os.path.exists(root_dir): continue
        for root, dirs, files in os.walk(root_dir):
            for file in files:
                # T√°mogatjuk a .json √©s a .jsonl form√°tumot is
                if file.endswith('_adatok.json') or file.endswith('.json') or file == 'knowledge_base.jsonl':
                    try:
                        path = os.path.join(root, file)
                        with open(path, 'r', encoding='utf-8') as f:
                            if file.endswith('.jsonl'):
                                # JSONL beolvas√°s soronk√©nt
                                for line in f:
                                    if line.strip():
                                        d = json.loads(line)
                                        d['source_type'] = 'REPO'
                                        d['origin'] = file
                                        docs.append(d)
                            else:
                                # Sima JSON t√∂mb beolvas√°s
                                chunk = json.load(f)
                                stype = 'ELM√âLET' if 'theory' in root_dir else 'K√ìD'
                                for d in chunk:
                                    d['source_type'] = stype
                                    d['origin'] = file
                                    docs.append(d)
                    except Exception as e:
                        log(f"Hiba a {file} olvas√°sakor: {e}")

    if not docs:
        log("KRITIKUS HIBA: √úresek az indexek! Ellen≈ërizd a let√∂lt√©st vagy a JSONL f√°jlt.")
        sys.exit(1)

    model = SentenceTransformer(MODEL_NAME)
    log(f"K√©sz. {len(docs)} dokumentum bet√∂ltve.")
    return docs, model

def hybrid_search(query, docs, model, top_k=TOP_K):
    """Kombin√°lt keres√©s: BM25 (Kulcssz√≥) + MPNET (Szemantika)"""

    # 1. BM25 (Kulcssz√≥ keres√©s)
    # Ez a leggyorsabb √©s legpontosabb a technikai szavakra (pl. CLR_GREEN)
    corpus = [(d.get('search_content') or d.get('content') or d.get('text') or '').lower().split() for d in docs]
    bm25 = BM25Okapi(corpus)
    bm25_scores = bm25.get_scores(query.lower().split())

    # Vegy√ºk a BM25 legjobb 50 tal√°lat√°t (el≈ësz≈±r√©s)
    candidates_idx = np.argsort(bm25_scores)[::-1][:50]
    candidates = [docs[i] for i in candidates_idx if bm25_scores[i] > 0]

    if not candidates:
        log("BM25 nem tal√°lt pontos egyez√©st. Pr√≥b√°lkoz√°s tiszt√°n szemantik√°val...")
        # Ha nincs kulcsszavas tal√°lat, akkor n√©zz√ºk √°t az eg√©szet az AI-val (lassabb, de tal√°lhat valamit)
        candidates = docs[:500] # Limit√°ljuk 500-ra a sebess√©g miatt, ha nincs index
        if not candidates: return []

    # 2. Szemantikus √öjrarangsorol√°s (Re-Ranking)
    # Az AI modell megn√©zi a jel√∂lteket, √©s kiv√°lasztja a kontextusban legjobbat
    query_vec = model.encode([query])
    candidate_texts = [c.get('search_content') or c.get('content') or c.get('text') for c in candidates]
    cand_embeddings = model.encode(candidate_texts)

    from sklearn.metrics.pairwise import cosine_similarity
    sim_scores = cosine_similarity(query_vec, cand_embeddings)[0]

    results = []
    for i, score in enumerate(sim_scores):
        results.append({'doc': candidates[i], 'score': score})

    results.sort(key=lambda x: x['score'], reverse=True)
    return results[:top_k]

def extract_next_steps(text):
    """Kinyeri a k√≥db√≥l, hogy mire kellene m√©g r√°keresni (Rekurzi√≥)."""
    next_queries = []
    # Include f√°jlok
    includes = re.findall(r'#include <(.*?)>', text)
    for inc in includes:
        clean_inc = inc.replace('\\', ' ').replace('.mqh', '')
        next_queries.append(f"MQL5 {clean_inc} content code")

    # Oszt√°lyok (C bet≈±vel kezd≈ëd≈ë PascalCase)
    classes = re.findall(r'\bC[A-Z][a-zA-Z0-9]+\b', text)
    for cls in classes:
        next_queries.append(f"MQL5 class {cls} definition usage")

    return list(set(next_queries))

def main():
    if len(sys.argv) < 2:
        print("Haszn√°lat: python kutato_ugynok.py \"<k√©rd√©s>\"")
        sys.exit(1)

    initial_query = ' '.join(sys.argv[1:])

    try:
        docs, model = load_resources()
    except Exception as e:
        print(f"Hiba a bet√∂lt√©sn√©l: {e}")
        sys.exit(1)

    print(f"\nüîé --- KUTAT√ÅS IND√çT√ÅSA: '{initial_query}' ---\n")

    knowledge_base = []
    queue = [initial_query]
    visited = set()

    for step in range(MAX_STEPS):
        if not queue: break
        current_q = queue.pop(0)

        # Normaliz√°l√°s a duplik√°ci√≥k elker√ºl√©s√©re
        q_sig = current_q.lower().strip()
        if q_sig in visited: continue

        log(f"L√©p√©s {step+1}: Kutat√°s erre: '{current_q}'")
        visited.add(q_sig)

        hits = hybrid_search(current_q, docs, model)

        if hits:
            log(f"   -> Tal√°ltam {len(hits)} relev√°ns inf√≥t.")
            for hit in hits:
                doc = hit['doc']
                # Csak akkor adjuk hozz√°, ha m√©g nincs benne
                if doc not in knowledge_base:
                    knowledge_base.append(doc)

                # √öj nyomok keres√©se (csak az els≈ë k√∂rben, hogy ne fusson v√©gtelenig)
                if step == 0:
                    content = doc.get('content') or doc.get('text') or ""
                    new_leads = extract_next_steps(content)
                    for lead in new_leads:
                        if lead.lower().strip() not in visited:
                            log(f"      -> √öj nyom (Automata): {lead}")
                            queue.append(lead)
        else:
            log("   -> Nincs tal√°lat.")

    # EREDM√âNYEK KI√çR√ÅSA
    print("\n" + "="*60)
    print("üìö --- √ñSSZEGY≈∞JT√ñTT TUD√ÅS (JULES SZ√ÅM√ÅRA) ---")
    print("="*60)

    unique_content = set()
    for i, doc in enumerate(knowledge_base):
        content = doc.get('content') or doc.get('text') or ""
        signature = content[:100] # Egyszer≈± duplik√°tum sz≈±r√©s az eleje alapj√°n
        if signature in unique_content: continue
        unique_content.add(signature)

        src_type = doc.get('source_type', 'RAG')
        fname = doc.get('filename') or doc.get('source') or '?'

        print(f"\nüìå [{i+1}] FORR√ÅS: {src_type} | F√ÅJL: {fname}")
        print("-" * 40)
        print(content[:1500] + ("\n... (folytat√°s a f√°jlban)" if len(content) > 1500 else ""))
        print("-" * 40)

if __name__ == "__main__":
    main()
